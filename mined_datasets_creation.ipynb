{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install python-docx\n",
        "!pip install find-job-titles\n",
        "!pip install Levenshtein\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFXI03ikVbI9",
        "outputId": "18b29c5d-b254-4374-9232-f4153bbcd42b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.10.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n",
            "Collecting find-job-titles\n",
            "  Downloading find_job_titles-0.7.0-py2.py3-none-any.whl (383 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.1/383.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting acora (from find-job-titles)\n",
            "  Downloading acora-2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (191 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.4/191.4 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from find-job-titles)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: acora, pyahocorasick, find-job-titles\n",
            "Successfully installed acora-2.4 find-job-titles-0.7.0 pyahocorasick-2.0.0\n",
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein) (3.6.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used this code to read the multiple documents and extract the text."
      ],
      "metadata": {
        "id": "EFcKakhmvvTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from docx import Document\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def identify_file_type(file_path):\n",
        "    resume = \"\"\n",
        "\n",
        "    file_name = file_path.split('/')[-1]  # Extracting the file name from the path\n",
        "    if file_name.endswith('.pdf'):\n",
        "        reader = PdfReader(file_path)\n",
        "        for i in range(len(reader.pages)):\n",
        "            page = reader.pages[i]\n",
        "            text = page.extract_text()\n",
        "            resume += text\n",
        "        return resume\n",
        "\n",
        "    elif file_name.endswith('.docx'):\n",
        "        document = Document(file_path)\n",
        "        for p in document.paragraphs:\n",
        "            resume += p.text\n",
        "        return resume\n",
        "\n",
        "    elif file_name.endswith('.html'):\n",
        "        with open(file_path, 'r', encoding='utf-8') as html_file:\n",
        "            soup = BeautifulSoup(html_file, 'html.parser')\n",
        "            for line in soup.get_text().splitlines():\n",
        "                resume += line\n",
        "        return resume\n",
        "\n",
        "    else:\n",
        "        return 'Unknown File Type'\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/documents20220826-1-v01cla.pdf\"  # Example file path\n",
        "resume = identify_file_type(file_path)\n",
        "print(\"File type:\", resume)"
      ],
      "metadata": {
        "id": "u2YFEMjHpx7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e57c62-1e7d-438b-b953-1c05173a1f70"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File type: Ashwini Kalidas Vedula\n",
            "Email ID: ashwini.vedula@nyu.edu |+1 (201)-616-1347 |LinkedIn: linkedin.com/in/ashwini-vedula\n",
            "Education\n",
            "New York University, New York, NY Sept 2021- May 2023\n",
            "Courant Institute of Mathematical Sciences and Stern School of Business\n",
            "Master of Science in Information Systems - 3.6/4\n",
            "Courses: Dealing with Data, Data Science for Business : Technical, Database Management Systems, Real Time\n",
            "Big Data Analytics, Cloud Computing\n",
            "Technical Skills\n",
            "Databases & Tools : MySQL, SSMS, MS Access, PostgreSQL, PL/SQL, MongoDB\n",
            "Data Analysis & Visualization Tools : Python, R , SQL, MS Excel, Tableau, Power BI\n",
            "Cloud Platforms & Big Data Technologies : AWS (Redshift, EC2, S3, EMR, SageMaker, DynamoDB Table), Azure\n",
            "(Storage, IoT, Virtual Machines), GCP (BigTable, DataLab), Apache Hadoop, Hive\n",
            "Experience\n",
            "Clark Associates Inc Pennsylvania, USA\n",
            "Financial Systems and Data Intern May 2022 - Aug 2022\n",
            "•Extracted data from varied data sources to automate Summary Income financial reports(monthly, quarterly and\n",
            "yearly) for the various subsidiaries of Clark Associates Inc. that reduced manual entry time by 70%.\n",
            "•Collaborated with cross-functional teams to enhance and optimize existing Power BI reports that helped stakeholders\n",
            "analyze the financial performance of their business.\n",
            "Dacapo Brokerage India Private Limited Mumbai,India\n",
            "Senior Analyst Aug 2020 - Jul 2021\n",
            "•Created visualization of daily top gainer/loser index stocks, relative sector performance and analyzed technical trends\n",
            "such as moving average of stock prices to help clients make informed investment decisions.\n",
            "CRISIL Limited, An S&P Global Company Mumbai,India\n",
            "Senior Associate - Data Science & Quants Lab Oct 2019 - Jul 2020\n",
            "•Lead a team of 2 interns to develop reports with functional specifications like KPI metrics, revenue tracking, budget\n",
            "vs. sales to understand opportunities in the sales funnel and improve operational effiency by 50% for the Senior\n",
            "Management of CRISIL’s Business Development Operations Team.\n",
            "Onsite Supply Chain Analytics Project - Doha, Qatar\n",
            "•Created and maintained 6 real time dashboards in Tableau that educated the Wholesale Banking Group on key\n",
            "metrics and performance measures.\n",
            "•Assisted in the design and development of Data Mart, fact tables using MySQL framework to decrease dependency\n",
            "on Tableau for complex calculations thereby reducing the dashboard loading time by 85%.\n",
            "•Exposure dealing with large relational data sets(8 Million+),load and query performance, archiving, stored\n",
            "procedures, etc.\n",
            "•Implemented a heuristic scoring model for approaching and on-boarding prospective customers that boosted\n",
            "conversions by 55%.\n",
            "•Built 31 cross-sell/up-sell rule-based configuration for product recommendations with Prescriptive Analytics.\n",
            "•Engaged with users in testing, release management, and operations to ensure quality of code development,\n",
            "deployment and post-production support.\n",
            "Associate Software Engineer - Data Science & Quants Lab Jul 2018 - Oct 2019\n",
            "•Determined sales forecasting for 250+ retainer stores of the biggest conglomerate in UAE using Holt-Winters\n",
            "algorithm.\n",
            "•Analysed data to develop a de-duplication algorithm model for grouping similar occurring brand names together.\n",
            "•Segmented 5 Million+ customers using RFM analysis to help the bank associates determine in advance the nature of\n",
            "future business operations and marketing strategies.\n",
            "•Performed Market Basket Analysis based on frequent purchasing patterns of customers.\n",
            "Leadership and Achievements\n",
            "•Conferred Quarterly Award ,(Service Excellence Award - Q4 2019) at Crisil Limited.\n",
            "•Received CRISILite Award for Performance (CLAP) , for the month of May 2019.\n",
            "•Treasurer of Computer Society of India -VESIT Student Chapter for the year 2016-2017.\n",
            "•Executive Head ofSponsorship Committee in the inter-collegiate technical festival.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is for creating the ONET dataset. The input is the original dataset and the output is a dataset with only the umbrella titles (those whose code ends with 00) included"
      ],
      "metadata": {
        "id": "GvE_qs1FwtzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# ONET dataframe\n",
        "df = pd.read_csv(\"2019_Occupations.csv\")\n",
        "\n",
        "# Function to check if a numeric code ends with '00'\n",
        "def ends_with_00(code):\n",
        "    return re.match(r'.*00$', str(code)) is not None\n",
        "\n",
        "# Filter dataframe to find strings where their associated numeric codes end with '00'\n",
        "ONET_final = df[df['O*NET-SOC 2019 Code'].apply(ends_with_00)]\n",
        "ONET_final.to_csv(\"ONET_final.csv\")"
      ],
      "metadata": {
        "id": "qojmBVOqctvr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we then ran this code for each resume and used the extracted job titles to create our initial job title dataset."
      ],
      "metadata": {
        "id": "mLOlK1Jwv3wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from find_job_titles import Finder\n",
        "import re\n",
        "finder = Finder()\n",
        "def find_jobs(resume):\n",
        "  return(list(finder.findall(resume)))\n",
        "\n",
        "def extract_job_strings(jobs):\n",
        "  f_jobs = []\n",
        "  job_pattern = r\"'([^']*)'\"\n",
        "  for job in jobs:\n",
        "    j = str(job)\n",
        "    job_extracted = str(re.findall(job_pattern, j))\n",
        "    if (job_extracted not in f_jobs):\n",
        "      f_jobs.append(job_extracted)\n",
        "    else:\n",
        "      continue\n",
        "  return f_jobs\n",
        "\n",
        "def remove_extra_characters(input_string):\n",
        "  cleaned_string = re.sub(r'\\W+', ' ', input_string)\n",
        "  return cleaned_string.strip()\n",
        "\n",
        "jobs_from_resume = find_jobs(resume)\n",
        "f_jobs = extract_job_strings(jobs_from_resume)\n",
        "final_jobs = []\n",
        "for j in f_jobs:\n",
        "  final_jobs.append(remove_extra_characters(j))\n",
        "\n",
        "print(final_jobs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35yWcwYI1G3C",
        "outputId": "ec481c03-980b-4915-f388-8aaaf7eabaca"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Associate', 'Senior Analyst', 'Senior Associate', 'Business Development', 'Associate Software Engineer', 'Student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the dataset into single word and multi word job title **dataframes**"
      ],
      "metadata": {
        "id": "koE91LPSwLZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataframe based on single and multiple word job titles\n",
        "df = pd.read_csv(\"job_titles_resume.csv\")\n",
        "single_word_df = df[df['Job Title'].apply(lambda x: len(x.split()) == 1)]\n",
        "multiple_word_df = df[df['Job Title'].apply(lambda x: len(x.split()) > 1)]\n",
        "\n",
        "# Save dataframes to separate CSV files\n",
        "single_word_df.to_csv('single_word_job_titles.csv', index=False)\n",
        "multiple_word_df.to_csv('multiple_word_job_titles.csv', index=False)\n",
        "\n",
        "print(\"Dataframes have been split and saved as CSV files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNaKJbIXSdpR",
        "outputId": "74e454b5-5368-49f0-debe-109605b2d91d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframes have been split and saved as CSV files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Levenshtein distance is used to map the single word job titles to the ONET job titles"
      ],
      "metadata": {
        "id": "oU3MXcE4wRtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Levenshtein import distance\n",
        "import pandas as pd\n",
        "\n",
        "# Sample job titles from resumes\n",
        "resume_jobs = pd.read_csv(\"/content/single_word_job_titles.csv\")\n",
        "resume_job_titles = resume_jobs[\"Job Title\"].tolist()\n",
        "\n",
        "# Sample job titles from ONET database\n",
        "onet_jobs = pd.read_csv(\"ONET_final.csv\")\n",
        "onet_job_titles = onet_jobs[\"O*NET-SOC 2019 Title\"].tolist()\n",
        "\n",
        "# Function to find the most similar job title using Levenshtein distance\n",
        "def find_most_similar(resume_title, onet_titles):\n",
        "    min_distance = float('inf')\n",
        "    most_similar_title = None\n",
        "    for onet_title in onet_titles:\n",
        "        dist = distance(resume_title.lower(), onet_title.lower())\n",
        "        if dist < min_distance:\n",
        "            min_distance = dist\n",
        "            most_similar_title = onet_title\n",
        "    return most_similar_title\n",
        "\n",
        "# Mapping of resume job titles to most similar ONET job titles\n",
        "resume_to_onet_mapping = {}\n",
        "for resume_title in resume_job_titles:\n",
        "    most_similar_onet_title = find_most_similar(resume_title, onet_job_titles)\n",
        "    resume_to_onet_mapping[resume_title] = most_similar_onet_title\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "mapping_df = pd.DataFrame(resume_to_onet_mapping.items(), columns=['Resume Job Title', 'Most Similar ONET Job Title'])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "mapping_df.to_csv('single_job_mappings.csv', index=False)\n",
        "\n",
        "print(\"Mapping saved to single_job_mappings.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN9LrgV7ntCh",
        "outputId": "6a1008b0-803e-46fa-e46c-faeebfd2b90d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapping saved to single_job_mappings.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used BERT to map the multi word job titles to the ONET job titles."
      ],
      "metadata": {
        "id": "LEXyZCCTwfx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the dictionary of job titles and their corresponding full forms\n",
        "job_titles = {\n",
        "    \"CEO\": \"Chief Executive Officer\",\n",
        "    \"CTO\": \"Chief Technology Officer\",\n",
        "    \"CFO\": \"Chief Financial Officer\",\n",
        "    \"COO\": \"Chief Operating Officer\",\n",
        "    \"CMO\": \"Chief Marketing Officer\",\n",
        "    \"CIO\": \"Chief Information Officer\",\n",
        "    \"CSO\": \"Chief Security Officer\"\n",
        "}\n",
        "\n",
        "# Function to generate BERT embeddings for a given text\n",
        "def get_bert_embeddings(text):\n",
        "    # Define words to exclude from embeddings\n",
        "    excluded_words = [\"intern\"]\n",
        "\n",
        "    # Remove excluded words from the text\n",
        "    cleaned_text = ' '.join(word for word in text.split() if word.lower() not in excluded_words)\n",
        "\n",
        "    # Tokenize the cleaned text\n",
        "    inputs = tokenizer(cleaned_text, return_tensors='pt', padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract embeddings from BERT's output\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
        "    return embeddings.numpy()\n",
        "\n",
        "# Function to compute cosine similarity between two vectors\n",
        "def compute_cosine_similarity(vector1, vector2):\n",
        "    return cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]\n",
        "\n",
        "# Load job titles from CSV files\n",
        "resume_data = pd.read_csv('/content/multiple_word_job_titles.csv')\n",
        "onet_data = pd.read_csv('/content/ONET_final.csv')\n",
        "\n",
        "# Extract job titles from dataframes\n",
        "resume_job_titles = resume_data['Job Title'].tolist()\n",
        "onet_job_titles = onet_data['O*NET-SOC 2019 Title'].tolist()\n",
        "\n",
        "# Generate BERT embeddings for job titles\n",
        "resume_embeddings = {title: get_bert_embeddings(title) for title in resume_job_titles}\n",
        "onet_embeddings = {title: get_bert_embeddings(title) for title in onet_job_titles}\n",
        "\n",
        "# Map each resume job title to ONET job titles based on similarity\n",
        "job_title_mapping = {}\n",
        "for resume_title, resume_embedding in resume_embeddings.items():\n",
        "    # Check if the job title contains \"software\"\n",
        "    if 'software' in resume_title.lower():\n",
        "        job_title_mapping[resume_title] = \"Software Developers\"\n",
        "    # Check if the resume title corresponds to any key in the dictionary\n",
        "    elif any(key in resume_title for key in job_titles.keys()):\n",
        "        # If yes, map it to \"Chief Executive\"\n",
        "        job_title_mapping[resume_title] = \"Chief Executive\"\n",
        "    else:\n",
        "        similarity_scores = {}\n",
        "        for onet_title, onet_embedding in onet_embeddings.items():\n",
        "            similarity_scores[onet_title] = compute_cosine_similarity(resume_embedding, onet_embedding)\n",
        "        # Find the ONET job title with the highest similarity score\n",
        "        best_match = max(similarity_scores, key=similarity_scores.get)\n",
        "        job_title_mapping[resume_title] = best_match\n",
        "\n",
        "# Convert the mapping dictionary to a DataFrame\n",
        "mapping_df = pd.DataFrame(job_title_mapping.items(), columns=['Resume Job Title', 'ONET Job Title'])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "mapping_df.to_csv('job_title_mapping.csv', index=False)\n",
        "print(\"mapping saved to job_title_mapping.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpazT3LawsYU",
        "outputId": "a624b1aa-f906-47f3-b1b4-7090c0e8755b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mapping saved to job_title_mapping.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then merged the two mapped dataframes together and further merged it with the ONET codes, creating out final dataset ONET_codes. This contains the resume job title, the onet matched job title and the respective code."
      ],
      "metadata": {
        "id": "trfbPjYwxcHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV files\n",
        "final_job_mapping = pd.read_csv(\"/content/Final Job Mapping.csv\")\n",
        "onet_final = pd.read_csv(\"/content/ONET_final.csv\")\n",
        "\n",
        "# Merge the two dataframes based on the common column \"ONET Job Title\"\n",
        "merged_df = pd.merge(final_job_mapping, onet_final, left_on=\"ONET Job Title\", right_on=\"Job Titles\", how=\"left\")\n",
        "\n",
        "# Drop the redundant \"Job Titles\" column\n",
        "merged_df.drop(columns=[\"Job Titles\"], inplace=True)\n",
        "\n",
        "# Save the merged dataframe to a new CSV file\n",
        "# merged_df.to_csv(\"merged.csv\", index=False)\n",
        "merged_df = merged_df.drop([\"Unnamed: 0\", \"Job Descriptions\"], axis=1)\n",
        "merged_df.to_csv(\"ONET_codes.csv\")"
      ],
      "metadata": {
        "id": "Uabik7o0ydoX"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}
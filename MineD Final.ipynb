{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install python-docx\n",
        "!pip install find-job-titles\n",
        "!pip install Levenshtein\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38yGk6NwXKvR",
        "outputId": "c26017d1-e438-4b6d-acb9-b255b31afdf2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.10.0)\n",
            "Requirement already satisfied: find-job-titles in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: acora in /usr/local/lib/python3.10/dist-packages (from find-job-titles) (2.4)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from find-job-titles) (2.0.0)\n",
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.10/dist-packages (0.25.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein) (3.6.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install -y antiword"
      ],
      "metadata": {
        "id": "YdgMorKobkLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a7cba2-52c5-461c-8b4c-c44da0a7daca"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connected to cloud.r-proj\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Connecting to ppa.launchpadcontent.net (185.125.190.8\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Connecting to ppa.launchpadcontent.net (185.125.190.80)] [Waiting for heade\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "antiword is already the newest version (0.37-16).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  This section converts every document type into strings (txt)\n",
        "\n",
        "\n",
        "(Docx , HTML, PDF , Doc)"
      ],
      "metadata": {
        "id": "PyJ-geVKhbcS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoplH4LJW8XN",
        "outputId": "46a74153-d4d6-4cc7-d258-7750c7ed09b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File type: Ashwini Kalidas Vedula\n",
            "Email ID: ashwini.vedula@nyu.edu |+1 (201)-616-1347 |LinkedIn: linkedin.com/in/ashwini-vedula\n",
            "Education\n",
            "New York University, New York, NY Sept 2021- May 2023\n",
            "Courant Institute of Mathematical Sciences and Stern School of Business\n",
            "Master of Science in Information Systems - 3.6/4\n",
            "Courses: Dealing with Data, Data Science for Business : Technical, Database Management Systems, Real Time\n",
            "Big Data Analytics, Cloud Computing\n",
            "Technical Skills\n",
            "Databases & Tools : MySQL, SSMS, MS Access, PostgreSQL, PL/SQL, MongoDB\n",
            "Data Analysis & Visualization Tools : Python, R , SQL, MS Excel, Tableau, Power BI\n",
            "Cloud Platforms & Big Data Technologies : AWS (Redshift, EC2, S3, EMR, SageMaker, DynamoDB Table), Azure\n",
            "(Storage, IoT, Virtual Machines), GCP (BigTable, DataLab), Apache Hadoop, Hive\n",
            "Experience\n",
            "Clark Associates Inc Pennsylvania, USA\n",
            "Financial Systems and Data Intern May 2022 - Aug 2022\n",
            "•Extracted data from varied data sources to automate Summary Income financial reports(monthly, quarterly and\n",
            "yearly) for the various subsidiaries of Clark Associates Inc. that reduced manual entry time by 70%.\n",
            "•Collaborated with cross-functional teams to enhance and optimize existing Power BI reports that helped stakeholders\n",
            "analyze the financial performance of their business.\n",
            "Dacapo Brokerage India Private Limited Mumbai,India\n",
            "Senior Analyst Aug 2020 - Jul 2021\n",
            "•Created visualization of daily top gainer/loser index stocks, relative sector performance and analyzed technical trends\n",
            "such as moving average of stock prices to help clients make informed investment decisions.\n",
            "CRISIL Limited, An S&P Global Company Mumbai,India\n",
            "Senior Associate - Data Science & Quants Lab Oct 2019 - Jul 2020\n",
            "•Lead a team of 2 interns to develop reports with functional specifications like KPI metrics, revenue tracking, budget\n",
            "vs. sales to understand opportunities in the sales funnel and improve operational effiency by 50% for the Senior\n",
            "Management of CRISIL’s Business Development Operations Team.\n",
            "Onsite Supply Chain Analytics Project - Doha, Qatar\n",
            "•Created and maintained 6 real time dashboards in Tableau that educated the Wholesale Banking Group on key\n",
            "metrics and performance measures.\n",
            "•Assisted in the design and development of Data Mart, fact tables using MySQL framework to decrease dependency\n",
            "on Tableau for complex calculations thereby reducing the dashboard loading time by 85%.\n",
            "•Exposure dealing with large relational data sets(8 Million+),load and query performance, archiving, stored\n",
            "procedures, etc.\n",
            "•Implemented a heuristic scoring model for approaching and on-boarding prospective customers that boosted\n",
            "conversions by 55%.\n",
            "•Built 31 cross-sell/up-sell rule-based configuration for product recommendations with Prescriptive Analytics.\n",
            "•Engaged with users in testing, release management, and operations to ensure quality of code development,\n",
            "deployment and post-production support.\n",
            "Associate Software Engineer - Data Science & Quants Lab Jul 2018 - Oct 2019\n",
            "•Determined sales forecasting for 250+ retainer stores of the biggest conglomerate in UAE using Holt-Winters\n",
            "algorithm.\n",
            "•Analysed data to develop a de-duplication algorithm model for grouping similar occurring brand names together.\n",
            "•Segmented 5 Million+ customers using RFM analysis to help the bank associates determine in advance the nature of\n",
            "future business operations and marketing strategies.\n",
            "•Performed Market Basket Analysis based on frequent purchasing patterns of customers.\n",
            "Leadership and Achievements\n",
            "•Conferred Quarterly Award ,(Service Excellence Award - Q4 2019) at Crisil Limited.\n",
            "•Received CRISILite Award for Performance (CLAP) , for the month of May 2019.\n",
            "•Treasurer of Computer Society of India -VESIT Student Chapter for the year 2016-2017.\n",
            "•Executive Head ofSponsorship Committee in the inter-collegiate technical festival.\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from docx import Document\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def identify_file_type(file_path):\n",
        "    resume = \"\"\n",
        "\n",
        "    file_name = file_path.split('/')[-1]  # Extracting the file name from the path\n",
        "    if file_name.endswith('.pdf'):\n",
        "        reader = PdfReader(file_path)\n",
        "        for i in range(len(reader.pages)):\n",
        "            page = reader.pages[i]\n",
        "            text = page.extract_text()\n",
        "            resume += text\n",
        "        return resume\n",
        "\n",
        "    elif file_name.endswith('.docx'):\n",
        "        document = Document(file_path)\n",
        "        for p in document.paragraphs:\n",
        "            resume += p.text\n",
        "        return resume\n",
        "\n",
        "    elif file_name.endswith('.html'):\n",
        "        with open(file_path, 'r', encoding='utf-8') as html_file:\n",
        "            soup = BeautifulSoup(html_file, 'html.parser')\n",
        "            for line in soup.get_text().splitlines():\n",
        "                resume += line\n",
        "        return resume\n",
        "\n",
        "    elif file_name.endswith('.doc'):\n",
        "      try:\n",
        "        process = subprocess.Popen(['antiword', file_path], stdout=subprocess.PIPE)\n",
        "        output, _ = process.communicate()\n",
        "        return output.decode('utf-8')\n",
        "\n",
        "      except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "    else:\n",
        "        return 'Unknown File Type'\n",
        "\n",
        "# Example usage:\n",
        "file_path = \"/content/documents20220826-1-v01cla.pdf\"  # Example file path\n",
        "resume = identify_file_type(file_path)\n",
        "print(\"File type:\", resume)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This section imports the pretrained model we have used from hugging face"
      ],
      "metadata": {
        "id": "slFaq493hp-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"has-abi/distilBERT-finetuned-resumes-sections\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"has-abi/distilBERT-finetuned-resumes-sections\")"
      ],
      "metadata": {
        "id": "UujGU8eQtUwa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This section splits the text into paragraphs\\lines"
      ],
      "metadata": {
        "id": "NC02J6W2hxA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize input\n",
        "input_text = resume.split(\"\\n\")\n",
        "print(input_text)"
      ],
      "metadata": {
        "id": "s6amX9I2xNfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e61ce56-508a-4725-e266-0cc89f5df60f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ashwini Kalidas Vedula', 'Email ID: ashwini.vedula@nyu.edu |+1 (201)-616-1347 |LinkedIn: linkedin.com/in/ashwini-vedula', 'Education', 'New York University, New York, NY Sept 2021- May 2023', 'Courant Institute of Mathematical Sciences and Stern School of Business', 'Master of Science in Information Systems - 3.6/4', 'Courses: Dealing with Data, Data Science for Business : Technical, Database Management Systems, Real Time', 'Big Data Analytics, Cloud Computing', 'Technical Skills', 'Databases & Tools : MySQL, SSMS, MS Access, PostgreSQL, PL/SQL, MongoDB', 'Data Analysis & Visualization Tools : Python, R , SQL, MS Excel, Tableau, Power BI', 'Cloud Platforms & Big Data Technologies : AWS (Redshift, EC2, S3, EMR, SageMaker, DynamoDB Table), Azure', '(Storage, IoT, Virtual Machines), GCP (BigTable, DataLab), Apache Hadoop, Hive', 'Experience', 'Clark Associates Inc Pennsylvania, USA', 'Financial Systems and Data Intern May 2022 - Aug 2022', '•Extracted data from varied data sources to automate Summary Income financial reports(monthly, quarterly and', 'yearly) for the various subsidiaries of Clark Associates Inc. that reduced manual entry time by 70%.', '•Collaborated with cross-functional teams to enhance and optimize existing Power BI reports that helped stakeholders', 'analyze the financial performance of their business.', 'Dacapo Brokerage India Private Limited Mumbai,India', 'Senior Analyst Aug 2020 - Jul 2021', '•Created visualization of daily top gainer/loser index stocks, relative sector performance and analyzed technical trends', 'such as moving average of stock prices to help clients make informed investment decisions.', 'CRISIL Limited, An S&P Global Company Mumbai,India', 'Senior Associate - Data Science & Quants Lab Oct 2019 - Jul 2020', '•Lead a team of 2 interns to develop reports with functional specifications like KPI metrics, revenue tracking, budget', 'vs. sales to understand opportunities in the sales funnel and improve operational effiency by 50% for the Senior', 'Management of CRISIL’s Business Development Operations Team.', 'Onsite Supply Chain Analytics Project - Doha, Qatar', '•Created and maintained 6 real time dashboards in Tableau that educated the Wholesale Banking Group on key', 'metrics and performance measures.', '•Assisted in the design and development of Data Mart, fact tables using MySQL framework to decrease dependency', 'on Tableau for complex calculations thereby reducing the dashboard loading time by 85%.', '•Exposure dealing with large relational data sets(8 Million+),load and query performance, archiving, stored', 'procedures, etc.', '•Implemented a heuristic scoring model for approaching and on-boarding prospective customers that boosted', 'conversions by 55%.', '•Built 31 cross-sell/up-sell rule-based configuration for product recommendations with Prescriptive Analytics.', '•Engaged with users in testing, release management, and operations to ensure quality of code development,', 'deployment and post-production support.', 'Associate Software Engineer - Data Science & Quants Lab Jul 2018 - Oct 2019', '•Determined sales forecasting for 250+ retainer stores of the biggest conglomerate in UAE using Holt-Winters', 'algorithm.', '•Analysed data to develop a de-duplication algorithm model for grouping similar occurring brand names together.', '•Segmented 5 Million+ customers using RFM analysis to help the bank associates determine in advance the nature of', 'future business operations and marketing strategies.', '•Performed Market Basket Analysis based on frequent purchasing patterns of customers.', 'Leadership and Achievements', '•Conferred Quarterly Award ,(Service Excellence Award - Q4 2019) at Crisil Limited.', '•Received CRISILite Award for Performance (CLAP) , for the month of May 2019.', '•Treasurer of Computer Society of India -VESIT Student Chapter for the year 2016-2017.', '•Executive Head ofSponsorship Committee in the inter-collegiate technical festival.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This section classifies the split strings into 12 predefined classes and prints the classified output"
      ],
      "metadata": {
        "id": "QeVod1N4iQnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "model_classes = {\n",
        "    0: \"awards\",\n",
        "    1: \"certificates\",\n",
        "    2: \"contact/name/title\",\n",
        "    3: \"education\",\n",
        "    4: \"interests\",\n",
        "    5: \"languages\",\n",
        "    6: \"para\",\n",
        "    7: \"professional_experiences\",\n",
        "    8: \"projects\",\n",
        "    9: \"skills\",\n",
        "    10: \"soft_skills\",\n",
        "    11: \"summary\"\n",
        "}\n",
        "\n",
        "final_output = {}\n",
        "skills_text = \"\"  # Initialize empty string to store skills text\n",
        "skills_list = []  # Initialize empty list to store skills paragraphs\n",
        "\n",
        "\n",
        "prof_pattern = r'\\b(analyst|engineer|developer|intern|system|data)\\b'\n",
        "\n",
        "certifications = []\n",
        "certification_patterns = [                              #Define a list of common certification abbreviations and patterns\n",
        "        r\"\\b([A-Za-z0-9\\s]+)\\b certification\\b\",\n",
        "        r\"\\bcertified (\\S+)\\b\",\n",
        "        r\"\\b([A-Za-z0-9\\s]+)\\b certified\\b\",\n",
        "        r\"\\b([A-Za-z0-9\\s]+)\\b certificate\\b\"\n",
        "    ]\n",
        "\n",
        "for inp in input_text:\n",
        "  inputs = tokenizer(inp, return_tensors = \"pt\")\n",
        "  output = model(**inputs)\n",
        "\n",
        "  output_logits = output.logits.tolist()\n",
        "  max_op = max(max(output_logits))\n",
        "\n",
        "  predicted_class_index = output_logits[0].index(max_op)\n",
        "  predicted_class_label = model_classes[predicted_class_index]\n",
        "\n",
        "  if predicted_class_label == \"awards\":\n",
        "    final_output.setdefault(\"awards\", []).append(inp)\n",
        "\n",
        "  elif predicted_class_label == \"certificates\":\n",
        "    if predicted_class_label == \"certificates\":\n",
        "      extracted_certificates = []\n",
        "      for pattern in certification_patterns:\n",
        "        matches = re.findall(pattern, inp, re.IGNORECASE)\n",
        "        if matches:\n",
        "          extracted_certificates.extend(matches)\n",
        "          final_output.setdefault(\"certificates\", []).extend(extracted_certificates)\n",
        "\n",
        "  elif predicted_class_label == \"contact/name/title\":\n",
        "    if re.findall(prof_pattern, inp, re.IGNORECASE):\n",
        "      final_output.setdefault(\"professional_experiences\", []).append(inp)\n",
        "    else:\n",
        "      final_output.setdefault(\"contact/name/title\", []).append(inp)\n",
        "\n",
        "  elif predicted_class_label == \"education\":\n",
        "        final_output.setdefault(\"education\", []).append(inp)\n",
        "\n",
        "  elif predicted_class_label == \"interests\":\n",
        "        final_output.setdefault(\"interests\", []).append(inp)\n",
        "\n",
        "  elif predicted_class_label == \"languages\":\n",
        "        final_output.setdefault(\"languages\", []).append(inp)\n",
        "\n",
        "  elif predicted_class_label == \"para\":\n",
        "        final_output.setdefault(\"para\", []).append(inp)\n",
        "\n",
        "  elif predicted_class_label == \"professional_experiences\":\n",
        "        final_output.setdefault(\"professional_experiences\", []).append(inp)\n",
        "\n",
        "  elif predicted_class_label == \"projects\":\n",
        "        final_output.setdefault(\"projects\", []).append(inp)\n",
        "\n",
        "  elif predicted_class_label == \"skills\":\n",
        "        final_output.setdefault(\"skills\", []).append(inp)\n",
        "        # Append paragraph to the skills list\n",
        "        skills_list.append(inp)\n",
        "\n",
        "  elif predicted_class_label == \"soft_skills\":\n",
        "        final_output.setdefault(\"soft_skills\", []).append(inp)\n",
        "\n",
        "  elif predicted_class_label == \"summary\":\n",
        "        final_output.setdefault(\"summary\", []).append(inp)\n",
        "\n",
        "# Join the skills list into a single string separated by commas\n",
        "skills_text = \", \".join(skills_list)\n",
        "\n",
        "for key, value in final_output.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "CI8B4FJSLwCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3da2e5-7df9-46c5-a5b8-7574e3a9553d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contact/name/title: ['Ashwini Kalidas Vedula', 'Email ID: ashwini.vedula@nyu.edu |+1 (201)-616-1347 |LinkedIn: linkedin.com/in/ashwini-vedula', 'Technical Skills', 'Clark Associates Inc Pennsylvania, USA', 'Dacapo Brokerage India Private Limited Mumbai,India', 'CRISIL Limited, An S&P Global Company Mumbai,India', 'Management of CRISIL’s Business Development Operations Team.', 'Onsite Supply Chain Analytics Project - Doha, Qatar', 'conversions by 55%.', 'algorithm.']\n",
            "education: ['Education', 'New York University, New York, NY Sept 2021- May 2023', 'Courant Institute of Mathematical Sciences and Stern School of Business', 'Master of Science in Information Systems - 3.6/4']\n",
            "skills: ['Big Data Analytics, Cloud Computing', 'Databases & Tools : MySQL, SSMS, MS Access, PostgreSQL, PL/SQL, MongoDB', 'Data Analysis & Visualization Tools : Python, R , SQL, MS Excel, Tableau, Power BI', 'Cloud Platforms & Big Data Technologies : AWS (Redshift, EC2, S3, EMR, SageMaker, DynamoDB Table), Azure', '(Storage, IoT, Virtual Machines), GCP (BigTable, DataLab), Apache Hadoop, Hive', 'metrics and performance measures.', '•Exposure dealing with large relational data sets(8 Million+),load and query performance, archiving, stored', '•Performed Market Basket Analysis based on frequent purchasing patterns of customers.']\n",
            "interests: ['Experience', 'future business operations and marketing strategies.']\n",
            "professional_experiences: ['Financial Systems and Data Intern May 2022 - Aug 2022', 'yearly) for the various subsidiaries of Clark Associates Inc. that reduced manual entry time by 70%.', 'Senior Analyst Aug 2020 - Jul 2021', 'Senior Associate - Data Science & Quants Lab Oct 2019 - Jul 2020', 'deployment and post-production support.', 'Associate Software Engineer - Data Science & Quants Lab Jul 2018 - Oct 2019', '•Determined sales forecasting for 250+ retainer stores of the biggest conglomerate in UAE using Holt-Winters']\n",
            "summary: ['•Collaborated with cross-functional teams to enhance and optimize existing Power BI reports that helped stakeholders', 'analyze the financial performance of their business.', '•Created visualization of daily top gainer/loser index stocks, relative sector performance and analyzed technical trends', 'such as moving average of stock prices to help clients make informed investment decisions.', '•Lead a team of 2 interns to develop reports with functional specifications like KPI metrics, revenue tracking, budget', 'vs. sales to understand opportunities in the sales funnel and improve operational effiency by 50% for the Senior', '•Created and maintained 6 real time dashboards in Tableau that educated the Wholesale Banking Group on key', '•Assisted in the design and development of Data Mart, fact tables using MySQL framework to decrease dependency', 'on Tableau for complex calculations thereby reducing the dashboard loading time by 85%.', '•Implemented a heuristic scoring model for approaching and on-boarding prospective customers that boosted', '•Built 31 cross-sell/up-sell rule-based configuration for product recommendations with Prescriptive Analytics.', '•Engaged with users in testing, release management, and operations to ensure quality of code development,', '•Analysed data to develop a de-duplication algorithm model for grouping similar occurring brand names together.', '•Segmented 5 Million+ customers using RFM analysis to help the bank associates determine in advance the nature of']\n",
            "soft_skills: ['Leadership and Achievements']\n",
            "awards: ['•Conferred Quarterly Award ,(Service Excellence Award - Q4 2019) at Crisil Limited.', '•Received CRISILite Award for Performance (CLAP) , for the month of May 2019.']\n",
            "para: ['•Treasurer of Computer Society of India -VESIT Student Chapter for the year 2016-2017.', '•Executive Head ofSponsorship Committee in the inter-collegiate technical festival.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_long_segments(text):\n",
        "    segments = text.split(',')\n",
        "    result = []\n",
        "    for segment in segments:\n",
        "        # Check if segment contains more than 4 words\n",
        "        if len(segment.split()) > 3:\n",
        "            # Check if the segment contains a colon, opening or closing parentheses, or an ampersand\n",
        "            if re.search(r':[^(]*\\)|\\(&\\)|&', segment):\n",
        "                result.append(segment)\n",
        "        else:\n",
        "            result.append(segment)\n",
        "    return ', '.join(result)\n",
        "\n",
        "# Example usage\n",
        "# ztext = \"Big Data Analytics, Cloud Computing Databases & Tools : MySQL, SSMS, MS Access, PostgreSQL, PL/SQL, MongoDB Data Analysis & Visualization Tools : Python, R , SQL, MS Excel, Tableau, Power BI Cloud Platforms & Big Data Technologies : AWS (Redshift, EC2, S3, EMR, SageMaker, DynamoDB Table), Azure (Storage, IoT, Virtual Machines), GCP (BigTable, DataLab), Apache Hadoop, Hive metrics and performance measures.\"\n",
        "filtered_text = remove_long_segments(skills_text)\n",
        "\n",
        "skills = filtered_text.split(\",\")\n",
        "for s in skills:\n",
        "  print(s)"
      ],
      "metadata": {
        "id": "P5YpyiBdfbSB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2004a7-61c3-4455-c44a-58a7df0c1d00"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Big Data Analytics\n",
            "  Cloud Computing\n",
            "  Databases & Tools : MySQL\n",
            "  SSMS\n",
            "  MS Access\n",
            "  PostgreSQL\n",
            "  PL/SQL\n",
            "  MongoDB\n",
            "  Data Analysis & Visualization Tools : Python\n",
            "  R \n",
            "  SQL\n",
            "  MS Excel\n",
            "  Tableau\n",
            "  Power BI\n",
            "  Cloud Platforms & Big Data Technologies : AWS (Redshift\n",
            "  EC2\n",
            "  S3\n",
            "  EMR\n",
            "  SageMaker\n",
            "  DynamoDB Table)\n",
            "  Azure\n",
            "  (Storage\n",
            "  IoT\n",
            "  Virtual Machines)\n",
            "  GCP (BigTable\n",
            "  DataLab)\n",
            "  Apache Hadoop\n",
            "  Hive\n",
            "  archiving\n",
            "  stored\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract jobs from the resume"
      ],
      "metadata": {
        "id": "V5vq3SuPf-Dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from find_job_titles import Finder\n",
        "# # Finding job titles\n",
        "# finder = Finder()\n",
        "# job_titles = finder.findall(resume)\n",
        "# print(\"Job titles found:\", job_titles)\n",
        "\n",
        "from find_job_titles import Finder\n",
        "import re\n",
        "finder = Finder()\n",
        "def find_jobs(resume):\n",
        "  return(list(finder.findall(resume)))\n",
        "\n",
        "def extract_job_strings(jobs):\n",
        "  f_jobs = []\n",
        "  job_pattern = r\"'([^']*)'\"\n",
        "  for job in jobs:\n",
        "    j = str(job)\n",
        "    job_extracted = str(re.findall(job_pattern, j))\n",
        "    if (job_extracted not in f_jobs):\n",
        "      f_jobs.append(job_extracted)\n",
        "    else:\n",
        "      continue\n",
        "  return f_jobs\n",
        "\n",
        "def remove_extra_characters(input_string):\n",
        "  cleaned_string = re.sub(r'\\W+', ' ', input_string)\n",
        "  return cleaned_string.strip()\n",
        "\n",
        "jobs_from_resume = find_jobs(resume)\n",
        "f_jobs = extract_job_strings(jobs_from_resume)\n",
        "final_jobs = []\n",
        "for j in f_jobs:\n",
        "  final_jobs.append(remove_extra_characters(j))\n",
        "\n",
        "print(final_jobs)"
      ],
      "metadata": {
        "id": "_GMc5em3fvbk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccffa82d-0786-42a2-c01d-7139eaacc8d8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Associate', 'Senior Analyst', 'Senior Associate', 'Business Development', 'Associate Software Engineer', 'Student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# returning ONET job codes"
      ],
      "metadata": {
        "id": "kdkI_zL2ibyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/ONET_codes.csv')\n",
        "\n",
        "for jobs in final_jobs:\n",
        "    matching_rows = df[df['Resume Job Title'] == jobs]\n",
        "    for row in matching_rows.itertuples(index=False):\n",
        "        print(\" - \".join(map(str, row[1:])))\n"
      ],
      "metadata": {
        "id": "fku-b9t1-qZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0ffa23-24f9-4377-9231-62a1f4468d7c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Associate - Chief Executives - 11-1011.00\n",
            "Senior Analyst - Sales Engineers - 41-9031.00\n",
            "Senior Associate - Editors - 27-3041.00\n",
            "Business Development - Financial Managers - 11-3031.00\n",
            "Associate Software Engineer - Software Developers - 15-1252.00\n",
            "Student - Tutors - 25-3041.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a career trajectory"
      ],
      "metadata": {
        "id": "sMGqg-u7uzeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_dates(text):\n",
        "    # Regular expression patterns to match dates in different formats\n",
        "    date_pattern1 = r'(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s\\d{4}'\n",
        "    date_pattern2 = r'\\b(?:19|20)\\d{2}\\b'\n",
        "\n",
        "    # Check for matches using both patterns\n",
        "    matches = re.findall(date_pattern1, text) + re.findall(date_pattern2, text)\n",
        "    return matches\n",
        "\n",
        "def extract_career_trajectory(resume_text):\n",
        "    # Extract dates from the resume text\n",
        "    experiences = resume_text.split(\"\\n\")\n",
        "\n",
        "    # Extract the dates associated with each experience\n",
        "    career_trajectory = []\n",
        "    for experience in experiences:\n",
        "        dates_in_experience = extract_dates(experience)\n",
        "        if dates_in_experience:\n",
        "            # Choose the first date encountered in each experience section\n",
        "            start_date = min(dates_in_experience)\n",
        "            career_trajectory.append((start_date, experience.strip()))\n",
        "\n",
        "    # Sort the career trajectory by starting dates\n",
        "    career_trajectory.sort(key=lambda x: x[0])\n",
        "\n",
        "    # Extract only the career trajectory text\n",
        "    career_trajectory_text = [experience[1] for experience in career_trajectory]\n",
        "\n",
        "    return career_trajectory_text\n",
        "\n",
        "# Example usage\n",
        "# Extract career trajectory\n",
        "career_trajectory = extract_career_trajectory(resume)\n",
        "\n",
        "# Print career trajectory\n",
        "for experience in career_trajectory:\n",
        "    print(experience)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb3b89YbvXHX",
        "outputId": "935393e5-6aee-472a-f0b4-0e4b20c0ffe5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "•Treasurer of Computer Society of India -VESIT Student Chapter for the year 2016-2017.\n",
            "Associate Software Engineer - Data Science & Quants Lab Jul 2018 - Oct 2019\n",
            "Senior Associate - Data Science & Quants Lab Oct 2019 - Jul 2020\n",
            "•Conferred Quarterly Award ,(Service Excellence Award - Q4 2019) at Crisil Limited.\n",
            "•Received CRISILite Award for Performance (CLAP) , for the month of May 2019.\n",
            "Senior Analyst Aug 2020 - Jul 2021\n",
            "New York University, New York, NY Sept 2021- May 2023\n",
            "Financial Systems and Data Intern May 2022 - Aug 2022\n"
          ]
        }
      ]
    }
  ]
}